import streamlit as st
import pandas as pd
import joblib
import re
import os
from src.clean_text import clean_text
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns

# -------------------------
# LOAD MODELS
# -------------------------
@st.cache_resource
def load_all_models():
    models = {
        "vectorizer": joblib.load("trained_models/tfidf_vectorizer.pkl"),

        "spam_clf": joblib.load("trained_models/spam_clf.pkl"),
        "spam_enc": joblib.load("trained_models/spam_encoder.pkl"),

        "freq_clf": joblib.load("trained_models/frequency_clf.pkl"),
        "freq_enc": joblib.load("trained_models/frequency_encoder.pkl"),

        "origin_clf": joblib.load("trained_models/origin_clf.pkl"),
        "origin_enc": joblib.load("trained_models/origin_encoder.pkl"),

        "price_clf": joblib.load("trained_models/price_clf.pkl"),
        "price_enc": joblib.load("trained_models/price_encoder.pkl"),

        "quality_clf": joblib.load("trained_models/quality_clf.pkl"),
        "quality_enc": joblib.load("trained_models/quality_encoder.pkl"),

        "service_clf": joblib.load("trained_models/service_clf.pkl"),
        "service_enc": joblib.load("trained_models/service_encoder.pkl"),
    }
    return models

models = load_all_models()

# -------------------------
# LOAD DATA
# -------------------------
DATA_PATH = "updated_reviews.csv"
@st.cache_data
def load_data():
    df = pd.read_csv(DATA_PATH)
    return df
df = load_data()

# Preprocess + Predict all dataset
def predict_all(df):
    clean_texts = df["content"].fillna("").apply(clean_text)
    vectors = models["vectorizer"].transform(clean_texts)

    df["spam_pred"] = models["spam_enc"].inverse_transform(models["spam_clf"].predict(vectors))
    df["frequency_pred"] = models["freq_enc"].inverse_transform(models["freq_clf"].predict(vectors))
    df["origin_pred"] = models["origin_enc"].inverse_transform(models["origin_clf"].predict(vectors))
    df["price_pred"] = models["price_enc"].inverse_transform(models["price_clf"].predict(vectors))
    df["quality_pred"] = models["quality_enc"].inverse_transform(models["quality_clf"].predict(vectors))
    df["service_pred"] = models["service_enc"].inverse_transform(models["service_clf"].predict(vectors))

    return df

df = predict_all(df)

# -------------------------
#  UI START
# -------------------------
st.title("üõí Product Review Analyzer ‚Äî Auto Updated Dashboard")

st.markdown("This dashboard displays the latest crawled review data along with predictions.")

# =========================
# Summary Statistics
# =========================
st.title("üìä Review Analytics Dashboard")

latest_time = pd.to_datetime(df['crawl_time']).max()
num_reviews = len(df)
num_customers = df['customer_id'].nunique() if 'customer_id' in df.columns else 0
num_sellers = df['seller_id'].nunique() if 'seller_id' in df.columns else 0

summary_df = pd.DataFrame({
    "Metric": [
        "Latest Crawl Time",
        "Number of Reviews",
        "Number of Customers",
        "Number of Sellers"
    ],
    "Value": [
        latest_time,
        num_reviews,
        num_customers,
        num_sellers
    ]
})

# ===== Custom column widths using HTML/CSS =====
st.subheader("üìå Summary")
st.markdown("""
<style>
.summary-table td:nth-child(1) { width: 200px !important; }
.summary-table td:nth-child(2) { width: 300px !important; }
</style>
""", unsafe_allow_html=True)

st.table(summary_df.style.set_table_attributes('class="summary-table"'))


# =========================
# Chart Grid: Class Distribution per Aspect
# =========================
st.subheader("üìä Class Distribution per Aspect")

aspects = ["spam_pred", "quality_pred", "service_pred", "origin_pred", "price_pred", "frequency_pred"]

n_cols = 3
# Loop through aspects in chunks of 3
for i in range(0, len(aspects), n_cols):
    row_aspects = aspects[i : i + n_cols]
    cols = st.columns(n_cols)

    for col, aspect in zip(cols, row_aspects):
        with col:
            fig, ax = plt.subplots()
            df[aspect].value_counts().plot(kind='bar', ax=ax)
            ax.set_title(f"Distribution of {aspect}")
            st.pyplot(fig)

# =========================
# Top Reviews per Class (2 √ó 2 tables)
# =========================
st.subheader("üåü Top Reviews (<30 words) by Class")

# Function to get short reviews
def get_short_reviews(df, column, label, n=5):
    subset = df[(df[column] == label) & (df['review'].str.split().str.len() < 30)]
    return subset[['review']].head(n)

for aspect in aspects:
    st.write(f"### üî∑ {aspect.replace('_pred','').title()}")

    # Detect classes dynamically from data
    aspect_classes = df[aspect].dropna().unique().tolist()

    # Sort classes for consistent layout
    priority_order = ["good", "positive", "yes", "neutral", "neu", "bad", "negative", "no", "na"]
    aspect_classes = sorted(aspect_classes, key=lambda x: priority_order.index(x) if x in priority_order else 999)

    # 2√ó2 layout
    c1, c2 = st.columns(2)

    # Fill each column with up to 2 classes
    left_classes = aspect_classes[:2]
    right_classes = aspect_classes[2:4]

    with c1:
        for cls in left_classes:
            st.markdown(f"#### {cls.upper()}")
            st.table(get_short_reviews(df, aspect, cls))

    with c2:
        for cls in right_classes:
            st.markdown(f"#### {cls.upper()}")
            st.table(get_short_reviews(df, aspect, cls))

    # If more than 4 classes, show remaining below
    if len(aspect_classes) > 4:
        st.warning(f"Aspect **{aspect}** has more than 4 classes. Showing extra below:")
        for cls in aspect_classes[4:]:
            st.markdown(f"#### {cls.upper()}")
            st.table(get_short_reviews(df, aspect, cls))


# =========================
# WordCloud
# =========================
st.subheader("‚òÅÔ∏è WordCloud of Customer Reviews")

text = " ".join(df['review'].astype(str).tolist())

wc = WordCloud(width=800, height=400).generate(text)

fig, ax = plt.subplots(figsize=(10, 5))
ax.imshow(wc, interpolation='bilinear')
ax.axis("off")
st.pyplot(fig)

# -------------------------
# Allow user to analyze one review
# -------------------------
st.subheader("üîç Analyze a Single Review")

review = st.selectbox("Pick a review:", df["content"].dropna().tail(100))
cleaned = clean_text(review)
vector = models["vectorizer"].transform([cleaned])

st.write({
    "Spam": models["spam_enc"].inverse_transform(models["spam_clf"].predict(vector))[0],
    "Frequency": models["freq_enc"].inverse_transform(models["freq_clf"].predict(vector))[0],
    "Origin": models["origin_enc"].inverse_transform(models["origin_clf"].predict(vector))[0],
    "Price": models["price_enc"].inverse_transform(models["price_clf"].predict(vector))[0],
    "Quality": models["quality_enc"].inverse_transform(models["quality_clf"].predict(vector))[0],
    "Service": models["service_enc"].inverse_transform(models["service_clf"].predict(vector))[0],
})